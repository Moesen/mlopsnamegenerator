{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.models.architectures import SimpleGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGPT.PokemonModel(\n",
    "    transformers_model=\"../models/exp1\", eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-198307a35bd09b0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/pheithar/.cache/huggingface/datasets/csv/default-198307a35bd09b0a/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5059.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 873.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/pheithar/.cache/huggingface/datasets/csv/default-198307a35bd09b0a/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 949.37it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\"test\": \"../data/processed/test.csv\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenize_function(tokenizer: GPT2Tokenizer, separator: str, max_length: int):\n",
    "    \"\"\"Function used when mapping dataset with a certain tokenization.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (GPT2Tokenizer): The object used to tokenize\n",
    "        separator (str): Special characters added so the model can\n",
    "            learn that what comes after is the name/answer\n",
    "    Returns:\n",
    "        (Callable): Tokenizer function used in dataset.map\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_function(text: Dataset):\n",
    "        \"\"\"Function to tokenize an input in format\n",
    "        [description] + [separator] + [name].\n",
    "\n",
    "        Args:\n",
    "            text (Dataset): Dataset with 'name' and 'description'\n",
    "                columns\n",
    "        \"\"\"\n",
    "        output = [\n",
    "            pkmn_desc + separator + pkmn_name\n",
    "            for pkmn_name, pkmn_desc in zip(text[\"name\"], text[\"description\"])\n",
    "        ]\n",
    "\n",
    "        results = tokenizer(output, max_length=max_length, padding=\"max_length\")\n",
    "        results[\"labels\"] = results[\"input_ids\"].copy()\n",
    "        return results\n",
    "\n",
    "    return tokenize_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.35ba/s]\n"
     ]
    }
   ],
   "source": [
    "token_function = get_tokenize_function(tokenizer, \" = /@\\ = \", max_length)\n",
    "tokenized_dataset = dataset.map(token_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "        output_dir=\"./tests\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        max_steps=max_steps,\n",
    "        seed=seed,\n",
    "        save_strategy=\"no\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        report_to=\"wandb\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b396eed8293b2cc7dbce7a6352df90ca6419036f9438b669431e1bcc2242ce22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ProjectMLOps': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
